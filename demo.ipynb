{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of final language-specific code - Turkish\n",
    "\n",
    "**LNG 409 Fall 2024**\n",
    "\n",
    "Author: Hee Joong Choi, Yuzhang Fu, Xin You\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pynini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walkthrough our final code named `transducer_tur.py` and/or `transducer_tur.ipynb`, which implements vowel harmony correction for Turkish noun plural and verb progressive suffixes using Finite-State Transducers (FSTs) via the `pynini` library. We will demonstrate the tweaks we made, the logistics of the code, and the performance of these transducers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this code, install pynini, a library for building and running weighted finite-state transducers.\n",
    "\n",
    "```Python\n",
    "!pip install pynini\n",
    "```\n",
    "\n",
    "**Note**: Throughout the project, we are only able import the pynini library on google colab. For instance, it is suggested to upload `transducer_tur.ipynb` along with `data/tur.out` and `data/tur.dev` to google drive and run the code. However, we have also attached `trasducer_tur.py` in our github repository for you to check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vowel categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created two helper functions that categorize vowels in Turkish words:\n",
    "\n",
    "`n_vowel_categorize(lemma)` determines the frontness and roundness of the last vowel in a lemma, where **'ö, ü'** are front rounded vowels, **'e,i'** are front unrounded vowels, **'o, u'** are back rounded vowels, and **'a, ı'** are back unrounded vowels.\n",
    "\n",
    "```python\n",
    "def n_vowel_categorize(lemma):\n",
    "    for char in reversed(lemma):\n",
    "        if char in \"öü\":\n",
    "            return {\"frontness\": True, \"roundness\": True}\n",
    "        elif char in \"ei\":\n",
    "            return {\"frontness\": True, \"roundness\": False}\n",
    "        elif char in \"ou\":\n",
    "            return {\"frontness\": False, \"roundness\": True}\n",
    "        elif char in \"aı\":\n",
    "            return {\"frontness\": False, \"roundness\": False}\n",
    "```\n",
    "\n",
    "`v_vowel_categorize(lemma)` finds the second-to-last vowel in the lemma to handle progressive suffix rules with the same vowel categorization rules above.\n",
    "\n",
    "```python\n",
    "def v_vowel_categorize(lemma):\n",
    "    vowels = \"öüeiouaı\"\n",
    "    n = 0\n",
    "    for char in reversed(lemma):\n",
    "        if char in vowels:\n",
    "            n += 1\n",
    "            if n == 2:\n",
    "                if char in \"öü\":\n",
    "                    return {\"frontness\": True, \"roundness\": True}\n",
    "                elif char in \"ei\":\n",
    "                    return {\"frontness\": True, \"roundness\": False}\n",
    "                elif char in \"ou\":\n",
    "                    return {\"frontness\": False, \"roundness\": True}\n",
    "                elif char in \"aı\":\n",
    "                    return {\"frontness\": False, \"roundness\": False}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plural Suffix Correction for Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of transducers\n",
    "```python\n",
    "turkish_alphabet = \"abcçdefgğhıijklmnoöprsştuüvyz \" # Added \" \"(space) to include compounds like \"sözdizimsel tuzlerde\"\n",
    "sigma = pynini.union(*turkish_alphabet).closure()\n",
    "```\n",
    "The Turkish alphabet is defined, including a space character to handle multi-word compounds. `sigma` is created to match any valid Turkish word.\n",
    "\n",
    "```python\n",
    "plural_correction_rule_a = pynini.cdrewrite(\n",
    "        pynini.cross(\"e\", \"a\"),\n",
    "        \"l\",\n",
    "        \"r\",\n",
    "        sigma)\n",
    "\n",
    "plural_correction_rule_e = pynini.cdrewrite(\n",
    "        pynini.cross(\"a\", \"e\"),\n",
    "        \"l\",\n",
    "        \"r\",\n",
    "        sigma)\n",
    "```\n",
    "Two rewrite rules are defined:\n",
    "1. **`plural_correction_rule_a`**: Converts *\"e\"* to *\"a\"* when the suffix should be *\"lar\"* (used for back vowels).\n",
    "2. **`plural_correction_rule_e`**: Converts *\"a\"* to *\"e\"* when the suffix should be *\"ler\"* (used for front vowels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for noun processing\n",
    "```python\n",
    "n_n_pl = 0 # counter for total number or words\n",
    "c_n_pl = 0 # counter for correct words\n",
    "\n",
    "for i, line in enumerate(tur_out):\n",
    "  lemma, msd, inflected = line[:3]\n",
    "  correct = dev[i]\n",
    "\n",
    "  if msd.startswith(\"N\") and '(PL' in msd:\n",
    "    v_cat = n_vowel_categorize(lemma)\n",
    "```\n",
    "First, we check for nouns and plural forms by creating a filter that only process entries with part-of-speech `N` (noun) and containing `(PL` (plural) in `msd`. Then, we categorize vowel harmony by using the `n_vowel_categorize` function to determine the vowel frontness of the lemma.\n",
    "\n",
    "```python\n",
    "    if v_cat[\"frontness\"] == False and \"ler\" in inflected:\n",
    "      output = inflected @ plural_correction_rule_a\n",
    "      paths = list(output.paths().ostrings())\n",
    "      correction = paths[0]\n",
    "      print(lemma, inflected, \"→\", correction, correct == correction)\n",
    "      n_n_pl += 1\n",
    "      if correct == correction:\n",
    "        c_n_pl += 1\n",
    "\n",
    "    elif v_cat[\"frontness\"] == True and \"lar\" in inflected:\n",
    "      output = inflected @ plural_correction_rule_e\n",
    "      paths = list(output.paths().ostrings())\n",
    "      correction = paths[0]\n",
    "      print(lemma, inflected, \"→\", correction, correct == correction)\n",
    "      n_n_pl += 1\n",
    "      if correct == correction:\n",
    "        c_n_pl += 1\n",
    "\n",
    "    else:\n",
    "      correction = inflected\n",
    "\n",
    "  print(\"--------------------------------\")\n",
    "  print(\"Number of words identified:\", n_n_pl)\n",
    "  print(\"Number of words corrected:\", c_n_pl)\n",
    "```\n",
    "Next, we apply plural suffix rules which correct `ler` to `lar` using `plural_correction_rule_a` for back vowels and correct `lar` to `ler` using `plural_correction_rule_e` for front vowels. In the end, we compare the corrected form with the (`correct`) and track the total (`n`) and successful corrections (`c`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample output\n",
    "The output shows each lemma, its original inflected form, the corrected form, and whether the correction matches the expected form (`True` or `False`).\n",
    "\n",
    "Example:\n",
    "```\n",
    "anahtarlık anahtarlıklerimize → anahtarlıklarimize False\n",
    "asimptot asimptotlerimizden → asimptotlarimizden False\n",
    "küçüklük küçüklüklar → küçüklükler True\n",
    "otel otellarını → otellerını False\n",
    "sözdizimsel tuz sözdizimsel tuzler → sözdizimsel tuzlar True\n",
    "```\n",
    "\n",
    "At the end, it prints the total processed cases (`n`) and successful corrections (`c`):\n",
    "```\n",
    "Number of words identified: 31\n",
    "Number of words corrected: 2\n",
    "```\n",
    "\n",
    "**2 out of 31** cases were corrected successfully. \n",
    "**Reason for low success rate**\n",
    "- While the plural suffix (lar/ler) is corrected, subsequent suffixes (e.g., possessive or case suffixes) remain unprocessed. These additional suffixes also require adjustments to align with vowel harmony rules, which the current implementation does not address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Suffix Correction for Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of transducers\n",
    "```python\n",
    "turkish_alphabet = \"abcçdefgğhıijklmnoöprsştuüvyzT \" # add \" \"(space) and T, so it includes word compounds and capital letter\n",
    "sigma = pynini.union(*turkish_alphabet).closure()\n",
    "vowel = pynini.union(*\"öüeiouaı\") # define vowels group\n",
    "consonant = pynini.union(*\"bcçdfgğhjklmnprsştvz\")  # define consonants group\n",
    "```\n",
    "Again, we start by defining the turkish alphabets, including space to handle multi-word compounds and the uppercase letter `T` for potential capitalized inputs. Additionally, a `vowel` is defined to represent the group of vowels while a `consonant` is defined to represent the group of consonants.\n",
    "\n",
    "```python\n",
    "# Progressive suffix context for adding ü\n",
    "prog_adding_rule_ü = pynini.cdrewrite(\n",
    "      pynini.cross(\"\", \"ü\"),\n",
    "      consonant,    # Left context: a consonant\n",
    "      \"yor\",    # Right context: before \"yor\"\n",
    "      sigma)\n",
    "\n",
    "# Progressive suffix context for adding i\n",
    "prog_adding_rule_i = pynini.cdrewrite(\n",
    "      pynini.cross(\"\", \"i\"),\n",
    "      consonant,    # Left context: a consonant\n",
    "      \"yor\",    # Right context: before \"yor\"\n",
    "      sigma)\n",
    "\n",
    "# Progressive suffix context for correcting back to ü\n",
    "prog_correction_rule_ü = pynini.cdrewrite(\n",
    "        pynini.cross(vowel, \"ü\"),\n",
    "        \"\",    # Left context: anything\n",
    "        \"yor\",    # Right context: before \"yor\" (equal to immediately followed by 'yor')\n",
    "        sigma)\n",
    "\n",
    "# Progressive suffix context for correcting back to i\n",
    "prog_correction_rule_i = pynini.cdrewrite(\n",
    "        pynini.cross(vowel, \"i\"),\n",
    "        \"\",    # Left context: anything\n",
    "        \"yor\",    # Right context: before \"yor\" (equal to immediately followed by 'yor')\n",
    "       sigma)\n",
    "```\n",
    "Two rewrite rules are created to handle corrections to the progressive suffix which ensure the **correction of improperly formed progressive suffixes**:\n",
    "1. **`prog_correction_rule_ü`**: Replaces any vowel with *\"ü\"* when followed by *\"yor\"*, ensuring proper alignment with front and rounded vowels.\n",
    "2. **`prog_correction_rule_i`**: Replaces any vowel with *\"i\"* in the same context, aligning with front and unrounded vowels.\n",
    "\n",
    "Addtionally, two rewrite rules are defined which ensure the **addition of missing vowels** to align with Turkish vowel harmony principles:\n",
    "1. **`prog_adding_rule_ü`**: Inserts *\"ü\"* when the preceding character is a consonant and the following context is *\"yor\"*. This rule aligns with front and rounded vowels.\n",
    "2. **`prog_adding_rule_i`**: Inserts *\"i\"* under the same context but applies when the vowel is front and unrounded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for verb processing\n",
    "```python\n",
    "n_v_prog = 0\n",
    "c_v_prog = 0\n",
    "\n",
    "# Process\n",
    "for i, line in enumerate(tur_out):\n",
    "  lemma, msd, inflected = line[:3]\n",
    "  correct = dev[i]\n",
    "\n",
    "  if msd.startswith(\"V\") and 'PROG' in msd:\n",
    "    v_cat = v_vowel_categorize(lemma)\n",
    "```\n",
    "First, we check for verbs and progressive forms by creating a filter that only process entries with part-of-speech `V` (verb) and containing `PROG` (progressive) in `msd`. Then, we categorize vowel harmony by using the `v_vowel_categorize` function to determine the vowel frontness of the lemma.\n",
    "\n",
    "```python\n",
    "    # first, adding missing vowel ü, then correcting\n",
    "    if v_cat[\"frontness\"] == True and v_cat[\"roundness\"] == True:\n",
    "      output = inflected @ prog_adding_rule_ü\n",
    "      output = output @ prog_correction_rule_ü\n",
    "      paths = list(output.paths().ostrings())\n",
    "      correction = paths[0]\n",
    "      print(lemma, inflected, \"→\", correction, correct == correction)\n",
    "      n_v_prog += 1\n",
    "      if correct == correction:\n",
    "        c_v_prog += 1\n",
    "\n",
    "    # first, adding missing vowel i, then correcting\n",
    "    elif v_cat[\"frontness\"] == True and v_cat[\"roundness\"] == False:\n",
    "      output = inflected @ prog_adding_rule_i\n",
    "      output = output @ prog_correction_rule_i\n",
    "      paths = list(output.paths().ostrings())\n",
    "      correction = paths[0]\n",
    "      print(lemma, inflected, \"→\", correction, correct == correction)\n",
    "      n_v_prog += 1\n",
    "      if correct == correction:\n",
    "        c_v_prog += 1\n",
    "\n",
    "    else:\n",
    "      correction = inflected\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"Number of words identified:\", n_v_prog)\n",
    "print(\"Number of words corrected:\", c_v_prog)\n",
    "```\n",
    "Next, we apply progressive suffix rules which correct or insert *\"i\"* or *\"ü\"* using four transducers designed before. In the end, we compare the corrected form with the (`correct`) and track the total (`n_v_prog`) and successful corrections (`c_v_prog`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output\n",
    "Same as above, the output shows each lemma, its original inflected form, the corrected form, and whether the correction matches the expected form (`True` or `False`).\n",
    "\n",
    "Example:\n",
    "```\n",
    "Türkçeleştirmek Türkçeleştirüyor muydu → Türkçeleştiriyor muydu True\n",
    "Türkçeleştirmek Türkçeleştirmiyormuşuz → Türkçeleştirmiyormuşuz True\n",
    "Türkçeleştirmek Türkçeleştiriyormuş → Türkçeleştiriyormuş True\n",
    "Türkçeleştirmek Türkçeleştiryor olmamalıymışım → Türkçeleştiriyor olmamalıymışım True\n",
    "Türkçeleştirmek Türkçeleştiriyor olmalıymışsın → Türkçeleştiriyor olmalıymışsın True\n",
    "```\n",
    "\n",
    "At the end, it prints the total processed cases (`n_v_prog`) and successful corrections (`c_v_prog`):\n",
    "```\n",
    "Number of words identified: 108\n",
    "Number of words corrected: 98\n",
    "```\n",
    "\n",
    "**98 out of 108** cases were corrected successfully. \n",
    "\n",
    "**Analysis for high accuracy improvement** \n",
    "- The transducers cover most of the errors shown in verbs with progressive forms. This case shows the importance of customizing transducers to cover more mistakes shown in the inflected words. We will then go back to enhance the plural suffix correction rules for nouns to increase the efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
